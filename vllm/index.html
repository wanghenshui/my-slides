<!doctype html>

<html>
<head>

    <title>vLLM介绍</title>

    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <title>reveal.js</title>

    <link rel="stylesheet" href="../reveal.js/dist/reset.css">
    <link rel="stylesheet" href="../reveal.js/dist/reveal.css">
    <link rel="stylesheet" href="../assets/white.css" id="theme">

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="../reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
    <div class="reveal">
        <div class="slides">
            <section data-markdown>
                <textarea data-template>
                    # vLLM
                    - 背景/动机
                    - 架构/设计
  			    </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    <!-- .element: style="font-size:80%;" -->
                    <h1 style="text-align: left;">背景</h1>
                    - llm推理需求显著，相关服务众多但效率低下
                    - 需要一个快速且方便使用/定制的llm推理服务

                    ```python
                    from vllm import LLM

                    # Example prompts.
                    prompts = ["Hello, my name is", "The capital of France is"]
                    # Create an LLM with HF model name.
                    llm = LLM(model="meta-llama/Meta-Llama-3.1-8B")
                    # Generate texts from the prompts. 
                    outputs = llm.generate(prompts) # also llm.chat(messages)]
                    ```
                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    <!-- .element: style="font-size:80%;" -->
                    <h3 style="text-align: left;">vllm接口</h2>
                    <h4 style="text-align: left;">server</h4>

                    ```shell
                    vllm serve meta-llama/Meta-Llama-3.1-8B
                    ```

                    <h4 style="text-align: left;">client</h4>

                    ```shell
                    curl http://localhost:8000/v1/completions \
                    -H "Content-Type: application/json" \
                    -d '{
                        "model": "meta-llama/Meta-Llama-3.1-8B",
                        "prompt": "San Francisco is a",
                        "max_tokens": 7,
                        "temperature": 0
                    }'

                    ```

                </textarea>
            </section>

            <section data-markdown>
                <textarea data-template>
                    <!-- .element: style="font-size:75%;" -->
                    <h1 style="text-align: left;">整体框架</h1>

                    ![](./vllm1.png) <!-- .element height="80%" width="80%" -->
                    
                    即便从V0演化到了分离式架构V1 组件还是这些
                </textarea>
            </section>


 
            <section>
                <h1>多进程架构交互</h1>
                <div class="mermaid">


                    <pre>
                        flowchart TD
                        subgraph "多进程架构"
                            API_SERVER["API Server进程<br>处理用户请求/分词/解码"]
                            ENGINE_CORE["EngineCore进程<br>调度/GPU推理"]
                            API_SERVER <--> |"ZeroMQ IPC<br>SyncMPClient/AsyncMPClient"| ENGINE_CORE
                        end
                    
                        subgraph "EngineCore内部结构"
                            CORE["EngineCore<br>core.py"]
                            SCHEDULER["Scheduler<br>core/scheduler.py"]
                            KV_CACHE_MGR["KVCacheManager<br>core/kv_cache_manager.py"]
                            EXECUTOR["ModelExecutor<br>executor/"]
                            
                            CORE --> SCHEDULER
                            CORE --> KV_CACHE_MGR
                            CORE --> EXECUTOR
                        end
                        
                        subgraph "Executor架构"
                            EXECUTOR_CLASS["ExecutorClass<br>executor/__init__.py"]
                            MULTIPROC_EXEC["MultiprocExecutor<br>executor/mp.py"]
                            WORKER_PROCS["Worker进程<br>WorkerProc.worker_main()"]
                            GPU_MODEL["GPUModelRunner<br>worker/gpu_model_runner.py"]
                            
                            EXECUTOR_CLASS --> MULTIPROC_EXEC
                            MULTIPROC_EXEC --> |"创建<br>make_worker_process()"| WORKER_PROCS
                            WORKER_PROCS --> GPU_MODEL
                        end
                        
                        ENGINE_CORE --- CORE
                </pre>
                  </div>
            </section>
                      

            <section>
                <h1>执行Engine初始化</h1>
                <div class="mermaid">


                    <pre>
                    flowchart TD
                    subgraph "EngineCore初始化<br>engine/core.py"
                        INIT_ENGINE["EngineCore.__init__()<br>引擎初始化"]
                        INIT_EXECUTOR["初始化Executor<br>executor_class.__init__()"]
                        AVAIL_MEM["determine_available_memory()<br>确定可用显存"]
                        GET_KV_CONFIG["get_kv_cache_config()<br>获取KV缓存配置"]
                        INIT_KV["initialize_from_config()<br>初始化KV缓存"]
                        INIT_SCHED["初始化Scheduler<br>Scheduler()"]
                    end
                    
                    subgraph "Executor初始化<br>执行者初始化"
                        MP_INIT["MultiprocExecutor._init_executor()<br>多进程执行者初始化"]
                        CREATE_MQ["创建消息队列<br>rpc_broadcast_mq"]
                        CREATE_WORKERS["创建Worker进程<br>make_worker_process()"]
                        WORKER_INIT["Worker进程初始化<br>worker_main()"]
                        INIT_DEVICE["初始化设备<br>init_device()"]
                        LOAD_MODEL["加载模型<br>load_model()"]
                    end
                    
                    subgraph "Worker KV缓存<br>worker/gpu_model_runner.py"
                        GPU_RUNNER["GPUModelRunner.__init__()<br>GPU模型运行器初始化"]
                        ALLOC_KV["分配KV缓存Tensor<br>每层2个(K/V)"]
                        KV_SHAPE["KV缓存形状<br>(2, num_blocks, block_size, kv_head, head_size)"]
                    end
                    
                    INIT_ENGINE --> INIT_EXECUTOR
                    INIT_ENGINE --> AVAIL_MEM
                    AVAIL_MEM --> GET_KV_CONFIG
                    GET_KV_CONFIG --> INIT_KV
                    INIT_KV --> INIT_SCHED
                    
                    INIT_EXECUTOR --> MP_INIT
                    MP_INIT --> CREATE_MQ
                    MP_INIT --> CREATE_WORKERS
                    CREATE_WORKERS --> WORKER_INIT
                    WORKER_INIT --> INIT_DEVICE
                    WORKER_INIT --> LOAD_MODEL
                    
                    LOAD_MODEL --> GPU_RUNNER
                    GPU_RUNNER --> ALLOC_KV
                    ALLOC_KV --> KV_SHAPE
                </pre>
                  </div>
            </section>

            <section>
                <h1>用户输入交互</h1>
                <div class="mermaid">


                    <pre>
                        flowchart LR
                        subgraph "Client进程<br>API Server"
                            PROMPT["用户输入Prompt"]
                            LLM_GEN["LLM.generate()<br>entrypoints/llm.py"]
                            ADD_REQ["_add_request()<br>LLMEngine.add_request()"]
                            TOKENIZE["processor.encode_request()<br>输入处理/分词"]
                            RUN_ENGINE["_run_engine()<br>while循环调用step"]
                            GET_OUT["LLMEngine.step()<br>engine_core.get_output()"]
                            DE_TOKEN["processor.process_output()<br>解码token"]
                            RESULT["返回结果给用户"]
                            
                            PROMPT --> LLM_GEN
                            LLM_GEN --> ADD_REQ
                            ADD_REQ --> TOKENIZE
                            LLM_GEN --> RUN_ENGINE
                            RUN_ENGINE --> GET_OUT
                            GET_OUT --> DE_TOKEN --> RESULT
                        end
                        
                        subgraph "EngineCore进程"
                            INPUT_Q["input_queue<br>EngineCoreProc成员"]
                            PROC_INPUT["process_input_socket线程<br>接收请求"]
                            BUSY_LOOP["run_busy_loop<br>主线程循环"]
                            HANDLE_REQ["_handle_client_request()<br>处理客户端请求"]
                            ADD_TO_SCHED["EngineCore.add_request()<br>添加到Scheduler"]
                            SCHED_STEP["EngineCore.step()<br>执行一次调度"]
                            OUTPUT_Q["output_queue<br>EngineCoreProc成员"]
                            PROC_OUTPUT["process_output_socket线程<br>发送结果"]
                            
                            PROC_INPUT --> INPUT_Q
                            INPUT_Q --> BUSY_LOOP
                            BUSY_LOOP --> HANDLE_REQ
                            HANDLE_REQ --> ADD_TO_SCHED
                            BUSY_LOOP --> SCHED_STEP
                            SCHED_STEP --> OUTPUT_Q
                            OUTPUT_Q --> PROC_OUTPUT
                        end
                        
                        ADD_REQ -->|"socket.send_multipart()<br>序列化请求"| PROC_INPUT
                        PROC_OUTPUT -->|"socket.recv_multipart()<br>反序列化结果"| GET_OUT
                </pre>
                  </div>
            </section>
            

        
            
            <section>
                <h1>kvcache管理初始化</h1>
                <div class="mermaid">
                    <pre>
                        flowchart TD
                        subgraph "KVCacheManager<br>core/kv_cache_manager.py"
                            INIT_KV["__init__()<br>初始化KVCache管理器"]
                            BLOCK_POOL["BlockPool<br>block_pool.py"]
                            REQ_BLOCKS["req_to_blocks<br>请求ID到KVCache块映射"]
                            REQ_HASH["req_to_block_hashes<br>请求ID到块Hash映射"]
                            
                            INIT_KV --> BLOCK_POOL
                            INIT_KV --> REQ_BLOCKS
                            INIT_KV --> REQ_HASH
                        end
                        
                        subgraph "BlockPool<br>core/block_pool.py"
                            INIT_BP["__init__()<br>创建所有KVCacheBlock"]
                            FREE_Q["free_block_queue<br>空闲块LRU链表"]
                            CACHED_MAP["cached_block_hash_to_block<br>Hash值到块的映射"]
                            ALLOC["allocate()<br>分配一个块"]
                            FREE["free()<br>释放一个块"]
                            
                            INIT_BP --> FREE_Q
                            INIT_BP --> CACHED_MAP
                            ALLOC --> FREE_Q
                            FREE --> FREE_Q
                        end
                        
                        subgraph "调度函数<br>core/scheduler.py"
                            GET_COMP["get_computed_blocks()<br>获取命中prefix cache的块"]
                            ALLOCATE["allocate_slots()<br>分配KVCache块"]
                            CACHE_FULL["cache_full_blocks()<br>缓存已满的块"]
                            
                            ALLOCATE --> |"调用"| CACHE_FULL
                        end
                        
                        GET_COMP --> |"查询"| REQ_HASH
                        GET_COMP --> |"返回命中的块"| ALLOCATE
                        ALLOCATE --> |"申请块<br>block_pool.allocate()"| ALLOC
                        ALLOCATE --> |"更新"| REQ_BLOCKS
                        CACHE_FULL --> |"计算hash并添加<br>cached_block_hash_to_block[hash]=block"| CACHED_MAP
                    
                    
                </pre>
                  </div>
            </section>
        

            <section>
                <section data-markdown>
                    <textarea data-template>
                        ## Q/A
                    </textarea>
                </section>

                <section data-markdown>
                    <textarea data-template>
                        ## 参考资料 {.left}

                        - https://github.com/vllm-project/vllm
                    </textarea>
                </section>
            </section>
        </div>
    </div>


    <script src="../reveal.js/dist/reveal.js"></script>
    <script src="../reveal.js/plugin/notes/notes.js"></script>
    <script src="../reveal.js/plugin/markdown/markdown.js"></script>
    <script src="../reveal.js/plugin/highlight/highlight.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js@5.0.5/dist/reveal.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@11.4.1/plugin/mermaid/mermaid.js"></script>
    <script>
        // More info about initialization & config:
        // - https://revealjs.com/initialization/
        // - https://revealjs.com/config/
        Reveal.initialize({
            hash: true,
            // PowerPoint 宽屏尺寸 (16:9)
            width: 1280,
            height: 720,
            
            // PowerPoint 标准尺寸 (4:3)
            // width: 1024, 
            // height: 768,
            
            // 控制内容缩放
            margin: 0.1,
            
            // 其他常用配置
            controls: true,
            progress: true,
            center: true,
            hash: true,
            scrollable: true,
            mermaid: {
            // flowchart: {
            //   curve: 'linear',
            // },
            },

            // Learn about plugins: https://revealjs.com/plugins/
            plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMermaid]
        });
    </script>
</body>

</html>
